# -*- coding: utf-8 -*-
"""Palluat_Pereira_Pedro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xToWE_pmt5OmZ7n6bhNde9YZPADdeaKA

# Apprenez une heuristique pour le Go

Dans ce **TP noté**, vous devrez déployer des methodes d'apprentissage automatique permettant d'évaluer la qualité de plateaux de GO.

Pour cela, vous disposerez de 21854 exemples de plateau de Go, tous générés par `gnugo` après quelques coups contre lui même avec un niveau de difficulté de 0. Par chaque plateau, nous avons lancé 100 matchs de gnugo contre lui même, toujours avec un niveau 0, et compté le nombre de victoires de noir et de blanc depuis ce plateau.

A noter, chaque "rollout" (un rollout et un déroulement possible du match depuis le plateau de référence) correspond à des mouvements choisis aléatoirement parmis les 10 meilleurs mouvements possibles, en biasant le choix aléatoire par la qualité prédite du mouvement par gnugo (les meilleurs mouvements ont une plus forte probabilité d'être tirés).

Les données dont vous disposez sont brutes. Ce sera à vous de proposer un format adéquat pour utiliser ces données en entrée de votre réseau neuronal.


## Comment sera évalué votre modèle ?

Nous vous fournirons 6h avant la date de rendu un nouveau fichier contenant 1000 nouveaux exemples, qui ne contiendront pas les champs `black_wins`, `white_wins`, `black_points` et `white_points`. Vous devrez laisser, dans votre dépot de projet (votre dépot GIT sous un sous-répertoire ML) un fichier texte nommé `my_predictions.txt` ayant une prédiction par ligne (un simple flottant) qui donnera, dans le même ordre de la liste des exemples les scores que vous prédisez pour chacune des entrées du fichier que nous vous aurons donné. ** Les scores seront donnés sous forme d'un flottant, entre 0 et 1, donnant la probabilité de victoire de noir sur le plateau considéré **. Il faudra laisser, dans votre feuille notebook (voir tout en dessous) la cellule Python qui aura créé ce fichier, pour que l'on puisse éventuellement refaire vos prédictions.

Bien entendu, vous nous rendrez également votre feuille jupyter **sous deux formats**, à la fois le fichier `.ipynb` et le fichier `.html` nous permettant de lire ce que vous avez fait, sans forcément relancer la feuille. Nous prendrons en compte les résultats obtenus sur les prédictions mais aussi le contenu de vos notebooks jupyter.

### Comment sera noté ce TP ?

**Il s'agit d'un TP noté (et non pas d'un projet), donc il ne faudra pas y passer trop de temps**. Nous attendons des prédictions correctes mais surtout des choix justifiés dans la feuille. Votre feuille notebook sera le plus important pour la notation (n'hésitez pas à mettre des cellules de texte pour expliquer pourquoi vous avez été amenés à faire certains choix). Ainsi, il serait bien d'avoir, par exemple, les données (graphiques ou autre) qui permettent de comprendre comment vous avez évité l'overfitting.

Le fichier de vos prédiction sera évalué en se basant sur la qualité de vos prédictions. Nous mesurerons par exemple (juste pour vous donner une idée) le nombre d'exemples dont votre prédiction donnera la bonne valeur à 5%, 10%, 20%, 35%, 50% pour estimer sa qualité.


## Mise en route !

Voyons  comment lire les données
"""

# Import du fichier d'exemples

def get_raw_data_go():
    ''' Returns the set of samples from the local file or download it if it does not exists'''
    import gzip, os.path
    import json

    raw_samples_file = "ppositions-to-evaluate-8x8-2025.json.gz"

    if not os.path.isfile(raw_samples_file):
        print("File", raw_samples_file, "not found, I am downloading it...", end="")
        import urllib.request
        urllib.request.urlretrieve ("https://www.labri.fr/perso/lsimon/static/inge2-ia/samples-8x8.json.gz", "samples-8x8.json.gz")
        print(" Done")

    with gzip.open("samples-8x8.json.gz") as fz:
        data = json.loads(fz.read().decode("utf-8"))
    return data

data = get_raw_data_go()
# print("We have", len(data),"examples")

"""## Compréhension des données de chaque entrée

Voici une description de chaque exemple
"""

def summary_of_example(data, sample_nb):
        ''' Gives you some insights about a sample number'''
        sample = data[sample_nb]
        print("Sample", sample_nb)
        print()
        print("Données brutes en format JSON:", sample)
        print()
        print("The sample was obtained after", sample["depth"], "moves")
        print("The successive moves were", sample["list_of_moves"])
        print("After these moves and all the captures, there was black stones at the following position", sample["black_stones"])
        print("After these moves and all the captures, there was white stones at the following position", sample["white_stones"])
        print("Number of rollouts (gnugo games played against itself from this position):", sample["rollouts"])
        print("Over these", sample["rollouts"], "games, black won", sample["black_wins"], "times with", sample["black_points"], "total points over all this winning games")
        print("Over these", sample["rollouts"], "games, white won", sample["white_wins"], "times with", sample["white_points"], "total points over all this winning games")

# summary_of_example(data,10)

"""## Données en entrée et en sortie de votre modèle final

Même si en interne, votre modèle va manipuler des tenseurs en numpy, vous devrez construire une boite noire qui prendra en entrée les données dans le style du JSON ci-dessous. Typiquement, vous aurez le même genre de fichier avec seulement les champs `black_stones`, `white_stones`, `depth` et `list_of_moves` de renseignées. Vous devrez utiliser ces champs, dont notemment les coordonnées des pierres noires et blanches et donner le pourcentage de chance pour noir de gagner depuis cette position.

Ainsi, pour l'exemple `i` :
- Vous pourrez prendez en entree `data[i]["black_stones"]` et `data[i]["white_stones"]` (vous pouvez, si vous le souhaitez, prendre en compte également `list_of_moves` ou tout autre donnée que vous calculerez vous-même (mais qui ne se basera évidemment pas sur les données que vous n'aurez pas lors de l'évaluation).
- Vous devrez prédire simplement `data[i]["black_wins"]/data[i]["rollouts"]` en float (donc une valeur entre 0 et 1).

Encore une fois, **attention** : en interne, il faudra absolument construire vos données formattées en matrices numpy pour faire votre entrainement. On vous demande juste ici d'écrire comment vous faites ces transformations, pour comprendre ce que vous avez décidé de mettre en entrée du réseau.

Voici par exemple le modèle de la fonction qui pourra être appelée, au final :

"""

# def position_predict(black_stones, white_stones):
#
#     # ... Votre tambouille interne pour placer les pierres comme il faut dans votre structure de données
#     # et appeler votre modèle Keras (typiquement avec model.predict())
#     prediction = None # model.predict(...) # A REMPLIR CORRECTEMENT
#
#     return prediction
#
# # Par exemple, nous pourrons appeler votre prédiction ainsi
#
# print("Prediction this sample:")
# summary_of_example(data, 10)
# print()
# prediction = position_predict(data[10]["black_stones"], data[10]["white_stones"])
# print("You predicted", prediction, "and the actual target was", data[10]["black_wins"]/data[10]["rollouts"])
#
# Ainsi, pour le rendu, en admettant que newdata soit la structure de données issue du json contenant les nouvelles données que
# l'on vous donnera 24h avant la fin, vous pourrez construire le fichier resultat ainsi

def create_result_file(newdata):
    ''' Exemple de méthode permettant de générer le fichier de resultats demandés. '''
    resultat  = [position_predict(d["black_stones"], d["white_stones"]) for d in newdata]
    with open("my_predictions.txt", "w") as f:
         for p in resultat:
            f.write(str(p)+"\n")

import matplotlib.pyplot as plt

if __name__=="__main__":
    plt.title("Relationship between the depth of the board and the chance for black to win")
    plt.plot([sample["black_wins"] for sample in data],[sample["depth"] for sample in data], '.')
    plt.xlabel("black wins (percentage)")
    plt.ylabel("depth of the game")


# Cumulative Distribution function of the chance of black to win
    cdf_wins = sorted([sample["black_wins"] for sample in data])
    plt.figure()
    plt.plot([x/len(cdf_wins) for x in range(len(cdf_wins))], cdf_wins)
    plt.grid()
    plt.title("Cumulative Distribution function of the chance of black to win")
    plt.xlabel("% of the samples with a chance of black to win below the y value")
    plt.ylabel("Chance of black to win")
    print("The CDF curve shows that black has more chances to win, globally")

"""# First steps: transform all the data into numpy arrays to feed your neural network

Advices:
- do not use only a 9x9 matrix as input. Use at least two planes to encode the board. One plane for black and one plane for white (typically with a 1 if there is a black stone for the first plane and with a 1 if there is a white stone for the second plane). The dimension of an input should be at least `[2,9,9]`. In Torch, the Conv2d method needs inputs as `[NBatch, Channels, H, W]`.
- consider to enrich your dataset with all symmetries and rotations. You should be able to multiply the number of samples to consider: any rotation of the board should have the same score, right?. You can use `np.rot90` to rotate your boards be beware of the dimensions (the channel is not the last dimension), so you may want to use `np.moveaxis()` to force the channels to be the last dimension, then call it again to make it the second one.
- what should happen on the score if you switch the colors? To know which player has to play next, you can check, for a sample, the parity of the length of the list `data[i]["list_of_moves"]` (an odd length list would mean that white is the next player. An even length list means that black has to play).
- work on enlarging and preparing your data only once. Once all you input data is setup as a big Numpy matrix, you may want to save it for speeding up everything. You can use, for instance `numpy.rot90()` and `numpy.flipud()` to generate all the symmetries


"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt

def name_to_coord(s):
    if s == "PASS":
        return None
    indexLetters = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':7}
    col = indexLetters[s[0]]
    row = int(s[1:]) - 1
    return row, col

def create_board_representation(sample):
    board = np.zeros((2, 8, 8), dtype=np.float32)

    for stone in sample['black_stones']:
        if stone != "PASS":
            row, col = name_to_coord(stone)
            if row is not None and col is not None:
                board[0, row, col] = 1

    for stone in sample['white_stones']:
        if stone != "PASS":
            row, col = name_to_coord(stone)
            if row is not None and col is not None:
                board[1, row, col] = 1

    return board

def prepare_data(data):
    boards = []
    labels = []

    for sample in data:
        original_board = create_board_representation(sample)
        label = sample["black_wins"] / sample["rollouts"]
        boards.append(original_board)
        labels.append(label)
        board_rot90 = np.rot90(original_board, k=1, axes=(1, 2))
        boards.append(board_rot90)
        labels.append(label)
        board_rot180 = np.rot90(original_board, k=2, axes=(1, 2))
        boards.append(board_rot180)
        labels.append(label)
        board_rot270 = np.rot90(original_board, k=3, axes=(1, 2))
        boards.append(board_rot270)
        labels.append(label)
        board_hflip = np.flip(original_board, axis=2)
        boards.append(board_hflip)
        labels.append(label)
        board_vflip = np.flip(original_board, axis=1)
        boards.append(board_vflip)
        labels.append(label)
        board_diag = np.transpose(original_board, (0, 2, 1))
        boards.append(board_diag)
        labels.append(label)
        board_antidiag = np.flip(np.transpose(original_board, (0, 2, 1)), axis=2)
        boards.append(board_antidiag)
        labels.append(label)

    return np.array(boards), np.array(labels)

"""# Second steps: build your neural network and train it

Don't forget to check overfitting, ...

*advices* :
- you may need to use some of the `torch.nn` layers: `Linear`, `Conv2d`, `ReLU`, `LeakyReLU`, `BatchNorm2d`, `Flatten`, `Dropout`... But you can of course first build a very simple one (and just pick some of them)...
- if you use convolution layers, be sure **not to downsize your board**. Applying a filter should keep the original size of the board (9x9), otherwise you would somehow forget the stones on the borders
- you will use like 33% of your input sample for validation. However, the final goal is to score new data that will be given in addition to the actual data. So, you should use the 33% splitting rule to set up your network architecture and, once you fixed it, you should train your final model on the whole set of data, crossing your fingers that it will generalize well.
- Warning: if you run a few epoch, and run it again for some more epochs, it will not reset the weights and the biases of your neural network. It's good news because you can add more and more epochs to your model, but be careful about the training/test sets (do split your sets before you initialize your model). Or you will be breaking your validation/training partition!

# Last step

Prepare your model to predict the set of new data to predict, you will have only 6 hours to push your predictions.

(may be you would like to express, when guessing the percentage of wins for blacks, that it should reflect the fact that this score should be the same for all the symmetries you considered)...

**Choix du modèle et explications**

Nous prenons un padding de 1 pour éviter de ne pas considérer les pions sur le côté de par la taille du tableau. Nous faisons une batchnormalization à chaque fois pour que l'apprentissage reste stable et un dropout pour ne pas avoir un surapprentissage. Nous prenons une sigmoid en sortie car elle permet de renvoyer des valeurs entre 0 et 1 ce qui correspond à une probabilité.
"""

'''Choix du modèle et explications

Nous prenons un padding de 1 pour éviter de ne pas considérer les pions sur le côté de par la taille
du tableau. Nous faisons une batchnormalization à chaque fois pour que l'apprentissage reste stable et un
dropout pour ne pas avoir un surapprentissage. Nous prenons une sigmoid en sortie car
 elle permet de renvoyer des valeurs entre 0 et 1 ce qui correspond à une probabilité.'''

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

class GoDataset(Dataset):
    def __init__(self, boards, labels):
        self.boards = torch.tensor(boards, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.float32).view(-1, 1)

    def __len__(self):
        return len(self.boards)

    def __getitem__(self, idx):
        return self.boards[idx], self.labels[idx]

'''Nous prenons des plusieurs couches de neuronnes plus profondes pour apprendre plus de patterns disponibles,
nous n'avons pas pris de trop gros layers pour ne pas avoir de trop grosses durées de calcul ainsi qu'un overfitting'''

class GoCNN(nn.Module):
    def __init__(self):
        super(GoCNN, self).__init__()
        self.conv1 = nn.Conv2d(2, 16, kernel_size=3, padding=1)
        self.bn1=nn.BatchNorm2d(16)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.bn2 =nn.BatchNorm2d(32)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.fc1 = nn.Linear(64 * 8 * 8, 128)

        self.fc2 = nn.Linear(128, 1)
        self.dropout= nn.Dropout(0.3)

    def forward(self, x):
        x = self.bn1(torch.relu(self.conv1(x)))
        x = self.bn2(torch.relu(self.conv2(x)))
        x = self.bn3(torch.relu(self.conv3(x)))
        x = x.view(-1, 64 * 8 * 8)
        x = torch.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))

        return x

def prepare_and_split_data(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):

    boards, labels = prepare_data(data)

    full_dataset = GoDataset(boards, labels)

    dataset_size = len(full_dataset)
    train_size = int(train_ratio * dataset_size)
    val_size = int(val_ratio * dataset_size)
    test_size = dataset_size - train_size - val_size

    train_dataset, val_dataset, test_dataset = random_split(
        full_dataset, [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(42)
    )

    return train_dataset, val_dataset, test_dataset

def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=20):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
        epoch_train_loss = running_loss / len(train_loader.dataset)
        train_losses.append(epoch_train_loss)
        model.eval()
        running_val_loss = 0.0
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                running_val_loss += loss.item() * inputs.size(0)
        epoch_val_loss = running_val_loss / len(val_loader.dataset)
        val_losses.append(epoch_val_loss)
        if scheduler is not None:
            scheduler.step(epoch_val_loss)
        if epoch_val_loss < best_val_loss:
            best_val_loss = epoch_val_loss
            torch.save(model.state_dict(), 'best_go_model.pth')
        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')
    model.load_state_dict(torch.load('best_go_model.pth'))
    return model, train_losses, val_losses

""" On regarde si cuda est disponible pour faire les calculs plus rapidement sinon on utilise le cpu"""

def evaluate_model(model, test_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()

    total_loss = 0.0
    criterion = nn.MSELoss()

    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            total_loss += loss.item() * inputs.size(0)

    avg_loss = total_loss / len(test_loader.dataset)
    print(f'Test Loss: {avg_loss:.4f}')
    return avg_loss

if __name__=="__main__":
    data = get_raw_data_go()
    train_dataset, val_dataset, test_dataset = prepare_and_split_data(data)
    batch_size = 128
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)
    test_loader = DataLoader(test_dataset, batch_size=batch_size)
    model = GoCNN()
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    model, train_losses, val_losses = train_model(
        model, train_loader, val_loader, criterion, optimizer, num_epochs=10
    )
    test_loss = evaluate_model(model, test_loader)
    torch.save(model.state_dict(), 'final_go_model.pth')

# position_predict = main()

def get_raw_data_go():
    ''' Returns the set of samples from the local file or download it if it does not exists'''
    import gzip, os.path
    import json

    raw_samples_file = "positions-to-evaluate-8x8-2025.json.gz"

    if not os.path.isfile(raw_samples_file):
        print("File", raw_samples_file, "not found, I am downloading it...", end="")
        import urllib.request
        urllib.request.urlretrieve ("https://www.labri.fr/perso/lsimon/static/inge2-ia/positions-to-evaluate-8x8-2025.json.gz", "positions-to-evaluate-8x8-2025.json.gz")
        print(" Done")

    with gzip.open("positions-to-evaluate-8x8-2025.json.gz") as fz:
        data = json.loads(fz.read().decode("utf-8"))
    return data

# data = get_raw_data_go()

def position_predict(black_stones, white_stones):
    """Prédit la probabilité de victoire de noir pour une position donnée"""
    model = GoCNN()
    model.load_state_dict(torch.load('best_go_model.pth'))

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()
    board = np.zeros((2, 8, 8), dtype=np.float32)

    for stone in black_stones:
        if stone != "PASS":
            coord = name_to_coord(stone)
            if coord is not None:
                row, col = coord
                if 0 <= row < 8 and 0 <= col < 8:
                    board[0, row, col] = 1
    for stone in white_stones:
        if stone != "PASS":
            coord = name_to_coord(stone)
            if coord is not None:
                row, col = coord
                if 0 <= row < 8 and 0 <= col < 8:
                    board[1, row, col] = 1
    board_tensor = torch.tensor(board, dtype=torch.float32).unsqueeze(0).to(device)

    with torch.no_grad():
        prediction = model(board_tensor).item()
        return prediction
def create_result_file(newdata):
    """Génère le fichier de résultats demandés"""
    predictions = []
    for i, sample in enumerate(newdata):
        prediction = position_predict(sample["black_stones"], sample["white_stones"])
        predictions.append(prediction)
    with open("my_predictions.txt", "w") as f:
        for p in predictions:
            f.write(str(p) + "\n")
    return predictions
# create_result_file(data)
